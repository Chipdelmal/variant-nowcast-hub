# Required fields
team_name: "Los Alamos National Laboratory"  # No more than 50 characters
team_abbr: "LANL"  # Up to 16 alphanumeric characters or underscores

model_name: "CovTransformer-v1.0"  # No more than 50 characters
model_abbr: "CovTransformer"  # Up to 16 alphanumeric characters or underscores

model_contributors:
  [
    {
      "name": "Michael Kupperman",
      "affiliation": "Theoretical Biology and Biophysics, Theoretical Division, Los Alamos National Laboratory",
      "email": "mkupperman@lanl.gov",
      "orcid": "0000-0003-0340-488X"  # Optional
    },
    {
      "name": "Yinan Feng",
      "affiliation": "School of Data Science and Society, University of North Carolina at Chapel Hill",
      "email": "contributor2@example.com",
      "orcid": "5678-5678-5678-5678"
    },
    {
      "name": "Youzuo Lin",
      "affiliation": "School of Data Science and Society, University of North Carolina at Chapel Hill",
      "email": "ylzin@unc.edu",
      "orcid": "0000-0001-7337-6760"
    },
    {
      "name": "Ruian Ke",
      "affiliation": "Theoretical Biology and Biophysics, Theoretical Division, Los Alamos National Laboratory",
      "email": "rke@lanl.gov",
      "orcid": "0000-0001-5307-8934"
    },
    {
      "name": "Emma Goldberg",
      "affiliation": "Theoretical Biology and Biophysics, Theoretical Division, Los Alamos National Laboratory",
      "email": "eegoldberg@lanl.gov",
      "orcid": "0000-0002-7311-9606"
    }
  ]

license: "CC-BY_SA-4.0"  # Choose from accepted licenses

team_funding:
  - "Laboratory Directed Research and and Development program at Los Alamos National Laboratory under Project number 20230830ER. National Institutes of Health, Grant number R01-AI152703 to Ruian Ke. The views expressed here are those of the authors and does not necessarily represent the official views of the National Institutes of Health, Los Alamos National Laboratory, or the US Government."

methods: "CovTransformer is a streamlined single-layer transformer architecture augmented with linear input and output layers, using embedding dimensions of 8 and dual attention heads. ÃŸThe model employs fixed sinusoidal positional embeddings and layer normalizations. An incremental prediction framework integrates short-term forecasts into long-term predictions through concatenation and shortcut connections. CovTransformer is an integrated ensemble of models, which first makes a 14 day prediction using 5 models (Stage 1 models). These predictions are averaged together, and passed back into another 5 fine-tuned transformers to improve long-term forcasts. The resulting 5 predictions are averaged, to form a single prediction for each lineage, for each day. To mitigate data imbalance, CovTransformer applies an exponential weighting scheme to a combined L1 and L2 loss function, prioritizing high-frequency labels essential for epidemiological and public health insights. The model was fit using the AdamW optimizer with cosine annealing, supplemented by noise injection. 5-fold cross-validation was used to train each model within the ensemble (using a different test-fold for each sub-model) on combined USA and GBR datasets. The ensemble outputs are unified into a single prediction by averaging. This model was trained on Pango lineage frequency time series data from GISAID (EPISET 240904ek) as of 2024/02/26. Sequences that did not have a country code or sampling date were removed. We created a series of retrospective databases for the preceeding 2 months at a 1-day resolution by eliminating sequences not-yet-submitted. This created a total of 145,419 time series between the USA and the United Kingdom. Training and test sets were assigned at the lineage level. We tested our model on NextClade lineages, and found no meaningful difference in performance. Inference is performed at the state level. To make a prediction, 14 days with data are required within the last 42 days. We exclude these lineages from our prediction. The total frequency is then normalized within each state over the lineages we do make a prediction for. For days with insufficient data, the NA predictions are normalized to zero. CovTransformer is licensed under BSD-3. The full license can be found at https://github.com/ruianke/CoVTransformer/blob/main/LICENSE."

methods_url: "https://www.medrxiv.org/content/10.1101/2024.04.01.24305089v1.full.pdf"  # Link to detailed write-up

data_sources:
  - "NextStrain (open build) for inference. GISAID (EPISET 240904ek) for training."

ensemble_of_models: true  # Set to true if it is an ensemble of separate models

ensemble_of_hub_models: false  # Set to true if it is an ensemble of hub-submitted models

# Optional fields
model_version: "v1.0"  # Optional version identifier

repo_url: "https://github.com/ruianke/CoVTransformer"  # Optional repository URL

citation:
- "Feng, Y., Goldberg, E. E., Kupperman, M., Zhang, X., Lin, Y., & Ke, R. (2024). CovTransformer: A transformer model for SARS-CoV-2 lineage frequency forecasting. Virus Evolution, to appear."
